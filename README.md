# TensorFlow_Serving_with_Docker_for_Model_Deployment
In this course, we  focus on three learning objectives:

Train and export TensorFlow Models for text classification  

Serve and deploy models with TensorFlow Serving and Docker  

Perform model inference with gRPC and REST endpoints  

This is a project on deploying deep learning models using TensorFlow Serving with Docker. In this project, you will train and export TensorFlow models for text classification, learn how to deploy models with TF Serving and Docker in 90 seconds, and build simple gRPC and REST-based clients in Python for model inference.  

With the worldwide adoption of machine learning and AI by organizations, it is becoming increasingly important for data scientists and machine learning engineers to know how to deploy models to production. While DevOps groups are fantastic at scaling applications, they are not the experts in ML ecosystems such as TensorFlow and PyTorch. This guided project gives learners a solid, real-world foundation of pushing your TensorFlow models from development to production in no time!  Prerequisites: In order to successfully complete this project, you should be familiar with Python, and have prior experience with building models with Keras or TensorFlow.
